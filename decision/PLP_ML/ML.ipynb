{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logits (y,batch_size,nb_label):\n",
    "    # Print a the label predicted\n",
    "    #Input : \n",
    "    #y : Numpy array [batch_size][10] The data predicted\n",
    "    #batch_size : Int The size of the batch\n",
    "    #nb_label : Int The number of labels\n",
    "    for i in range (0,batch_size):\n",
    "        print(\"------------\",i,\"------------\")\n",
    "        for j in range (0,nb_label):\n",
    "            if (y[i][j] != 0):\n",
    "                print(\"pr√©diction : \",j,\"pourcentage de certitude :\",\"%.1f\" % (y[i][j]*100) , \" %\")\n",
    "\n",
    "def int_to_dummies(i):\n",
    "    # Create a label for the number given\n",
    "    #Input: int The value of the label\n",
    "    result = [0]*30\n",
    "    result[int(i)] = 1\n",
    "    return np.array(result)\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "  ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def prediction_logits (y,batch_size,nb_label):\n",
    "    # Normalize the batch of label to obtain a percentage\n",
    "    #Input : \n",
    "    #y : Numpy array [batch_size][10] The data predicted\n",
    "    #batch_size : Int The size of the batch\n",
    "    #nb_label : Int The number of labels\n",
    "    #Out : Numpy array [batch_size][10] the input normalized\n",
    "    \n",
    "    y_sum = np.sum(y, axis=1)\n",
    "    for i in range (0,batch_size):\n",
    "        for j in range (0,nb_label):\n",
    "            if (y_sum[i] != 0):\n",
    "                y[i][j] = y[i][j]/y_sum[i]\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"parsed_data.csv\")\n",
    "df = df[df.ID_FTR != -1]\n",
    "target = df[\"ID_FTR\"]\n",
    "features = df[df.columns[7:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAG</th>\n",
       "      <th>AAS</th>\n",
       "      <th>AASO</th>\n",
       "      <th>AAF</th>\n",
       "      <th>AAC</th>\n",
       "      <th>AAY</th>\n",
       "      <th>AAR</th>\n",
       "      <th>HAG</th>\n",
       "      <th>HAS</th>\n",
       "      <th>HASO</th>\n",
       "      <th>HAF</th>\n",
       "      <th>HAC</th>\n",
       "      <th>HAY</th>\n",
       "      <th>HAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.235243</td>\n",
       "      <td>0.266572</td>\n",
       "      <td>-0.346070</td>\n",
       "      <td>-0.780578</td>\n",
       "      <td>2.451612</td>\n",
       "      <td>-0.994774</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>-0.962820</td>\n",
       "      <td>0.382529</td>\n",
       "      <td>0.074035</td>\n",
       "      <td>0.052537</td>\n",
       "      <td>-0.837804</td>\n",
       "      <td>-1.454220</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.564010</td>\n",
       "      <td>-0.170714</td>\n",
       "      <td>-0.995201</td>\n",
       "      <td>0.056229</td>\n",
       "      <td>-0.109168</td>\n",
       "      <td>-0.994774</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>-0.165427</td>\n",
       "      <td>0.049468</td>\n",
       "      <td>0.074035</td>\n",
       "      <td>0.052537</td>\n",
       "      <td>0.978304</td>\n",
       "      <td>-1.454220</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.034495</td>\n",
       "      <td>2.015716</td>\n",
       "      <td>1.384945</td>\n",
       "      <td>1.032504</td>\n",
       "      <td>0.256657</td>\n",
       "      <td>-0.994774</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>-0.165427</td>\n",
       "      <td>-1.393795</td>\n",
       "      <td>-0.794851</td>\n",
       "      <td>-0.798350</td>\n",
       "      <td>-1.564248</td>\n",
       "      <td>-1.936414</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.564010</td>\n",
       "      <td>0.922501</td>\n",
       "      <td>0.086683</td>\n",
       "      <td>0.893036</td>\n",
       "      <td>-0.109168</td>\n",
       "      <td>-1.478684</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>0.233269</td>\n",
       "      <td>-1.060734</td>\n",
       "      <td>-0.577630</td>\n",
       "      <td>1.612498</td>\n",
       "      <td>-1.564248</td>\n",
       "      <td>0.956749</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.164384</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>0.086683</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>0.622483</td>\n",
       "      <td>1.424779</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>-1.361516</td>\n",
       "      <td>-1.171754</td>\n",
       "      <td>-1.880959</td>\n",
       "      <td>0.619796</td>\n",
       "      <td>-0.111361</td>\n",
       "      <td>-0.007638</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1952</td>\n",
       "      <td>-0.963637</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>0.086683</td>\n",
       "      <td>-0.362175</td>\n",
       "      <td>-0.474994</td>\n",
       "      <td>-0.510863</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>1.429358</td>\n",
       "      <td>1.159671</td>\n",
       "      <td>1.811808</td>\n",
       "      <td>0.903425</td>\n",
       "      <td>0.615083</td>\n",
       "      <td>-0.007638</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1953</td>\n",
       "      <td>-0.564010</td>\n",
       "      <td>-0.389356</td>\n",
       "      <td>-0.562447</td>\n",
       "      <td>0.753568</td>\n",
       "      <td>-0.109168</td>\n",
       "      <td>0.456958</td>\n",
       "      <td>1.142606</td>\n",
       "      <td>0.631965</td>\n",
       "      <td>1.048651</td>\n",
       "      <td>0.942921</td>\n",
       "      <td>1.187054</td>\n",
       "      <td>-0.474582</td>\n",
       "      <td>0.474556</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1954</td>\n",
       "      <td>0.235243</td>\n",
       "      <td>-0.389356</td>\n",
       "      <td>0.086683</td>\n",
       "      <td>1.032504</td>\n",
       "      <td>-1.206645</td>\n",
       "      <td>0.456958</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>-1.361516</td>\n",
       "      <td>-0.172572</td>\n",
       "      <td>-0.577630</td>\n",
       "      <td>0.619796</td>\n",
       "      <td>-0.474582</td>\n",
       "      <td>0.474556</td>\n",
       "      <td>1.180820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1955</td>\n",
       "      <td>0.634869</td>\n",
       "      <td>0.485215</td>\n",
       "      <td>0.086683</td>\n",
       "      <td>0.893036</td>\n",
       "      <td>-0.840820</td>\n",
       "      <td>0.940868</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>-0.165427</td>\n",
       "      <td>0.604570</td>\n",
       "      <td>0.942921</td>\n",
       "      <td>-1.223794</td>\n",
       "      <td>-0.111361</td>\n",
       "      <td>-0.489832</td>\n",
       "      <td>-0.576492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1956</td>\n",
       "      <td>2.233375</td>\n",
       "      <td>-0.061392</td>\n",
       "      <td>1.601322</td>\n",
       "      <td>1.311440</td>\n",
       "      <td>0.256657</td>\n",
       "      <td>1.908689</td>\n",
       "      <td>-0.580106</td>\n",
       "      <td>0.631965</td>\n",
       "      <td>1.714772</td>\n",
       "      <td>0.291257</td>\n",
       "      <td>-0.089277</td>\n",
       "      <td>-0.474582</td>\n",
       "      <td>-0.489832</td>\n",
       "      <td>1.180820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1957 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AAG       AAS      AASO       AAF       AAC       AAY       AAR  \\\n",
       "0     0.235243  0.266572 -0.346070 -0.780578  2.451612 -0.994774 -0.580106   \n",
       "1    -0.564010 -0.170714 -0.995201  0.056229 -0.109168 -0.994774 -0.580106   \n",
       "2     1.034495  2.015716  1.384945  1.032504  0.256657 -0.994774 -0.580106   \n",
       "3    -0.564010  0.922501  0.086683  0.893036 -0.109168 -1.478684 -0.580106   \n",
       "4    -0.164384  0.047929  0.086683  0.614100  0.622483  1.424779 -0.580106   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1952 -0.963637 -0.061392  0.086683 -0.362175 -0.474994 -0.510863 -0.580106   \n",
       "1953 -0.564010 -0.389356 -0.562447  0.753568 -0.109168  0.456958  1.142606   \n",
       "1954  0.235243 -0.389356  0.086683  1.032504 -1.206645  0.456958 -0.580106   \n",
       "1955  0.634869  0.485215  0.086683  0.893036 -0.840820  0.940868 -0.580106   \n",
       "1956  2.233375 -0.061392  1.601322  1.311440  0.256657  1.908689 -0.580106   \n",
       "\n",
       "           HAG       HAS      HASO       HAF       HAC       HAY       HAR  \n",
       "0    -0.962820  0.382529  0.074035  0.052537 -0.837804 -1.454220 -0.576492  \n",
       "1    -0.165427  0.049468  0.074035  0.052537  0.978304 -1.454220 -0.576492  \n",
       "2    -0.165427 -1.393795 -0.794851 -0.798350 -1.564248 -1.936414 -0.576492  \n",
       "3     0.233269 -1.060734 -0.577630  1.612498 -1.564248  0.956749 -0.576492  \n",
       "4    -1.361516 -1.171754 -1.880959  0.619796 -0.111361 -0.007638 -0.576492  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1952  1.429358  1.159671  1.811808  0.903425  0.615083 -0.007638 -0.576492  \n",
       "1953  0.631965  1.048651  0.942921  1.187054 -0.474582  0.474556 -0.576492  \n",
       "1954 -1.361516 -0.172572 -0.577630  0.619796 -0.474582  0.474556  1.180820  \n",
       "1955 -0.165427  0.604570  0.942921 -1.223794 -0.111361 -0.489832 -0.576492  \n",
       "1956  0.631965  1.714772  0.291257 -0.089277 -0.474582 -0.489832  1.180820  \n",
       "\n",
       "[1957 rows x 14 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = features.iloc[1850:].values\n",
    "y_test = target.iloc[1850:].values\n",
    "\n",
    "x_train = features.iloc[:1850].values\n",
    "y_train = target.iloc[:1850].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1850 samples, validate on 107 samples\n",
      "Epoch 1/10\n",
      "1536/1850 [=======================>......] - ETA: 0s - loss: 1.1504 - accuracy: 0.3639\n",
      "Epoch 00001: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 2s 944us/sample - loss: 1.1436 - accuracy: 0.3692 - val_loss: 1.0768 - val_accuracy: 0.4766\n",
      "Epoch 2/10\n",
      "1248/1850 [===================>..........] - ETA: 0s - loss: 1.0824 - accuracy: 0.4335\n",
      "Epoch 00002: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 100us/sample - loss: 1.0849 - accuracy: 0.4281 - val_loss: 1.0666 - val_accuracy: 0.4766\n",
      "Epoch 3/10\n",
      "1632/1850 [=========================>....] - ETA: 0s - loss: 1.0691 - accuracy: 0.4375\n",
      "Epoch 00003: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 110us/sample - loss: 1.0698 - accuracy: 0.4335 - val_loss: 1.0525 - val_accuracy: 0.4953\n",
      "Epoch 4/10\n",
      "1440/1850 [======================>.......] - ETA: 0s - loss: 1.0631 - accuracy: 0.4465\n",
      "Epoch 00004: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 101us/sample - loss: 1.0602 - accuracy: 0.4508 - val_loss: 1.0607 - val_accuracy: 0.4766\n",
      "Epoch 5/10\n",
      "1632/1850 [=========================>....] - ETA: 0s - loss: 1.0508 - accuracy: 0.4577\n",
      "Epoch 00005: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 123us/sample - loss: 1.0548 - accuracy: 0.4541 - val_loss: 1.0572 - val_accuracy: 0.4766\n",
      "Epoch 6/10\n",
      " 896/1850 [=============>................] - ETA: 0s - loss: 1.0480 - accuracy: 0.4576\n",
      "Epoch 00006: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 84us/sample - loss: 1.0488 - accuracy: 0.4578 - val_loss: 1.0608 - val_accuracy: 0.4766\n",
      "Epoch 7/10\n",
      "1440/1850 [======================>.......] - ETA: 0s - loss: 1.0388 - accuracy: 0.4632\n",
      "Epoch 00007: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 118us/sample - loss: 1.0438 - accuracy: 0.4622 - val_loss: 1.0668 - val_accuracy: 0.4673\n",
      "Epoch 8/10\n",
      "1824/1850 [============================>.] - ETA: 0s - loss: 1.0415 - accuracy: 0.4567\n",
      "Epoch 00008: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 150us/sample - loss: 1.0404 - accuracy: 0.4578 - val_loss: 1.0734 - val_accuracy: 0.4486\n",
      "Epoch 9/10\n",
      "1248/1850 [===================>..........] - ETA: 0s - loss: 1.0376 - accuracy: 0.4647\n",
      "Epoch 00009: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 142us/sample - loss: 1.0364 - accuracy: 0.4595 - val_loss: 1.0746 - val_accuracy: 0.4486\n",
      "Epoch 10/10\n",
      "1728/1850 [===========================>..] - ETA: 0s - loss: 1.0287 - accuracy: 0.4740\n",
      "Epoch 00010: saving model to models/model1.ckpt\n",
      "1850/1850 [==============================] - 0s 146us/sample - loss: 1.0321 - accuracy: 0.4703 - val_loss: 1.0746 - val_accuracy: 0.4579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f572dea848>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "checkpoint_path = \"models/model1.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "model.fit(x_train,y_train, epochs=10,validation_data=(x_test,y_test),\n",
    "          callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "------------ 0 ------------\n",
      "pr√©diction :  0 pourcentage de certitude : 22.5  %\n",
      "pr√©diction :  1 pourcentage de certitude : 22.3  %\n",
      "pr√©diction :  2 pourcentage de certitude : 55.2  %\n"
     ]
    }
   ],
   "source": [
    "index = np.random.randint(0,107)\n",
    "print(y_test[index])\n",
    "print_logits(model.predict(x = x_test[index].reshape(1,14), batch_size = 1),1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x = x_train)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "conf_mat = tf.math.confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
